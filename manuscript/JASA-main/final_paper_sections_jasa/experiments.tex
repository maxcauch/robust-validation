% -*- Mode: latex -*- %

\section{Empirical analysis}
\label{sec:exps}

Given the challenges arising in the practice of machine learning and
statistics, this paper argues that methodology equipping models with a
notion of validity in their predictions---\eg, conformalization procedures
as in this paper---is essential to any modern prediction pipeline.  Section
\ref{sec:motivation-exp} illustrates the need for these sorts of procedures,
showing that the standard conformal methodology is sensitive to even small
shifts in the data, through (semi-synthetic) experiments on data from the
UCI repository.  In Section~\ref{sec:robustpredinference}, we propose
methods for robust predictive inference, giving methodology that estimates
the amount of shift to which we should be robust.  Fuller justification
requires a more careful empirical study that highlights both the failures of
non-robust prediction sets on real data as well as the potential to handle
such shifts using the methodology here.  To that end, we turn to
experimental work:

\begin{itemize}
\item Section~\ref{sec:real-experiments} shows
  evaluation centered around the new MNIST, CIFAR-10, and ImageNet test
  sets. These datasets exhibit real-world distributional
  shifts, and we test whether our methodology of estimating
  plausible shifts is sufficient to provide coverage in these real-world shifts.
  \item In Appendix~\ref{sec:uci-experiments},
  we resume the evaluation of our own methodology on the
  semi-synthetic data from Section~\ref{sec:motivation-exp}.
\item In Appendix~\ref{sec:real-experiments-covid}, we consider a time
  series where the goal is to predict the fraction of people testing
  positive for COVID-19 throughout the United States.
\item In Appendix~\ref{sec:covariate-sensitivity},
  we apply Algorithm~\ref{alg:sensitivity1} to evaluate the sensitivity
  of predictive methods to individual covariate shifts.
\end{itemize}

\subsection{CIFAR-10, MNIST, and ImageNet datasets}
\label{sec:real-experiments}

We evaluate our procedures on the CIFAR-10,
ImageNet, and MNIST datasets~\cite{KrizhevskyHi09,
  RussakovskyDeSuKrSaMaHuKaKhBeBeFe15, LeCunCoBu98}, which
continue to play a central role in the evaluation of machine learning
methodology.
Concerns about overfitting to these benchmarks
motivate \citet*{RechtRoScSh19} to create new test sets for both
CIFAR-10 and ImageNet by carefully following the original dataset creation
protocol. Though these new test sets strongly resemble the original
datasets, as \citeauthor{RechtRoScSh19} observe, the natural variation
arising in the creation of the new test sets yield evidently significant
differences, giving organic dataset shifts on which to
evaluate our procedures.  Our goal here is to show that even when we do not know the actual amount of shift, our methodology from Section~\ref{sec:coverage-high-probability-over-shifts} can still give reasonable estimates of it that translate into marginal coverage on these datasets.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{cifar-10/cifar-10_coverages_boxplot.pdf}
    \hfill
    \includegraphics[scale=0.5]{cifar-10/cifar-10_sizes_boxplot.pdf}
    \caption{Empirical coverage and average size for the prediction sets
    generated by the standard conformal methodology (``SC'') and the chi-squared
    divergence, across 20 random splits of the CIFAR-10 data.  We set $\rho$
    according to the sampling (``$\chi^2$-S''), regression (``$\chi^2$-R''), and
    classification-based (``$\chi^2$-C'') strategies for estimating the amount
    of shift that we describe in Section in \ref{sec:futureshiftestimation}.
    The horizontal red line marks the marginal coverage $.95$.}
    \label{fig:cifar-10}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{mnist/mnist_coverages_boxplot.pdf}
    \hfill
    \includegraphics[scale=0.5]{mnist/mnist_sizes_boxplot.pdf}
    \caption{Empirical coverage and average size for the prediction sets
    generated by the standard conformal methodology (``SC'') and the chi-squared
    divergence, across 20 random splits of the MNIST data.  We set $\rho$
    according to the sampling (``$\chi^2$-S''), regression (``$\chi^2$-R''), and
    classification-based (``$\chi^2$-C'') strategies for estimating the amount
    of shift that we describe in Section in \ref{sec:futureshiftestimation}. The
    horizontal red line marks the marginal coverage $.95$.}
    \label{fig:mnist}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{imagenet/imagenet_coverages_boxplot.pdf}
    \hfill
    \includegraphics[scale=0.5]{imagenet/imagenet_sizes_boxplot.pdf}
    \caption{Empirical coverage and average size for the prediction sets
    generated by the standard conformal methodology (``SC'') and the chi-squared
    divergence, across 20 random splits of the ImageNet data.  We set $\rho$
    according to the sampling (``$\chi^2$-S''), regression (``$\chi^2$-R''), and
    classification-based (``$\chi^2$-C'') strategies for estimating the amount
    of shift that we describe in Section in \ref{sec:futureshiftestimation}. The
    horizontal red line marks the marginal coverage $.9$.}
    \label{fig:imagenet}
\end{figure}

We evaluate on the three datasets as follows.  We use 70\% of the
original CIFAR-10, MNIST, and ImageNet datasets for training, and treat the
remaining 30\% as a validation set.  We fit a standard
ResNet-50~\cite{HeZhReSu16} to the training data, and use the negative
log-likelihood $\score(x, y) = -\log p_\theta(y \mid x)$, where $p_\theta(y
\mid x)$ is the output of the (top) sigmoid layer of the network, as the
scoring function on the validation data for our conformalization procedures.
We compare our procedures to the split conformal methodology on three new
datasets nominally generated identically to the initial datasets: the
CIFAR-10.1~v4 dataset~\cite{RechtRoScSh19}, which consists of 2,000 
32$\times$32 images from 10 different classes; the QMNIST50K data,
which extends MNIST to consist of 50,000 28$\times$28 images
from 10 classes~\cite{YadavBo19}; and the ImageNetV2 Threshold0.7
data~\cite{RechtRoScSh19}, consisting of 10,000 images from 200 classes.  In
each test of robust predictive inference, we set the level of robustness to achieve the nominal coverage $\alpha = .05$ for the CIFAR-10 and MNIST datasets and $\alpha = .1$ for the ImageNet dataset, by using the data-driven strategies that we detail in
Section~\ref{sec:futureshiftestimation}: sampling directions of shift from
the uniform distribution on the unit sphere
(Alg.~\ref{alg:rho-selection-procedure}), estimating the shift direction via
regression (Alg.~\ref{alg:worst-direction-validation}) or via
classification, which replaces the regression step in
Alg.~\ref{alg:worst-direction-validation} with a support vector machine
(SVM) to separate the largest 50\% of scores $\score(X_i, Y_i)$ from the
smallest.
In contrast to our experiments from Section~\ref{sec:uci-experiments} with semi-synthetic data, we cannot compute the exact level of shift here; the
question is whether the provided methodology provides marginal coverage.
% Unlike our previous semi-synthetic experiments, these image classification tasks feature natural distributional shifts, which we cannot compute exactly but hope to capture.  Our goal is thus to find dataset-wise levels of robustness $\rho$ that achieve coverage at level $\alpha = .05$ on the CIFAR-10 and MNIST datasets and $\alpha = .1$ on the ImageNet dataset.

Figures \ref{fig:cifar-10}, \ref{fig:mnist}, and \ref{fig:imagenet} present
the results for each setup over 20 random splits of the data. As is apparent
from the figures, we see that the standard conformal methodology fails to
correctly cover.  As both the new CIFAR-10 and ImageNet test sets exhibit
larger degradations in classifier performance (increased error) than does
the MNIST test set~\cite{RechtRoScSh19}, we expect the failure of standard
conformal to be pronounced on these two datasets.  Indeed, the split
conformal method (Sec.~\ref{sec:split-conformal-intro}) provides especially
poor coverage on these datasets, where it yields average coverage .88
(instead of the nominal .95) and .8 (instead of the nominal .9) on the new
CIFAR-10 and ImageNet test sets, respectively.  On the other hand, our
inferential methodology consistently gives more coverage regardless of the
strategy used to estimate the amount of divergence $\rho$, with the sampling
strategy notably consistently delivering marginal coverage without
over-covering.  The uniformity in coverage across the three strategies is
notable, as our procedures for estimating the amount of shift assume some
structure for the underlying shift, which is unlikely to be consistent with
the provenance of the new test sets.

In our experiments, estimating the direction of shift using either
regression or classification (Alg.~\ref{alg:worst-direction-validation}) is
faster than sampling directions (Alg.~\ref{alg:rho-selection-procedure});
the former takes time $O(nd \min\{n, d\})$ and
the latter $O(k n d)$, where $k$ is the number
of sampled directions $v$, using a linear-time implementation for
computing the worst coverage (maximum density segment) along
a direction $v$~\cite{ChungLu03}.
The difference of course depends on the desired sampling frequency
$k$.

%% For example, on the MNIST data,
%% Algorithm~\ref{alg:worst-direction-validation} takes two seconds on a
%% workstation with dual Intel Xeon E5-2699 v4 processors and 4 GB of RAM,
%% while the sampling procedure Alg.~\ref{alg:rho-selection-procedure} with $k
%% = 5000$ takes roughly 14 minutes.  However, with a careful implementation
%% using the fast linear time algorithm of \citet{TODO}, it is likely that
%% Algorithm~\ref{alg:rho-selection-procedure} would outperform
%% Algorithm~\ref{alg:worst-direction-validation}, especially in high
%% dimensions.

Finally, the aforementioned validity does not (apparently) come
with a significant loss in statistical efficiency:
Figures~\ref{fig:cifar-10}, \ref{fig:mnist}, and \ref{fig:imagenet} show
that our confidence sets are not substantially larger than those coming from
standard conformal inference---which may be somewhat surprising, given the
relatively large number of classes present in the ImageNet dataset.

%\subsection{COVID-19 forecasting}
%\label{sec:real-experiments-covid}
%
%Our final evaluation of prediction accuracy under shifts is to predict test
%positivity rates for COVID-19, in each of $L = 3{,}140$ United States
%counties in a time series over $T = 34$ weeks from January through the
%beginning of August 2021, using demographic features.  As a
%non-stationary time series, robustness is essential here as a fixed model of
%course cannot adapt to the underlying distributional changes.
%
%Our prediction task is as follows.  For each of $t=1,\ldots,T$ weeks, and at
%each of $\ell=1,\ldots,L$ locations (counties), we observe a
%real-valued response $Y_{\ell, t} \in [0,1]$, $\ell=1,\ldots,L$,
%$t=1,\ldots,T$, measuring the fraction of people with COVID-19.
%We use data from the DELPHI group at Carnegie Mellon University
%\citep{ArnoldBiBrCoFaGrMaReTi21,Tibshirani20} and consider a similar
%featurization, using the following
%trailing average features within each county: (1)
%the number of COVID-19 cases per 100{,}000 people; (2) the number of doctor
%visits for COVID-like symptoms; and (3) the number of responses
%to a Facebook survey indicating respondents have COVID-like symptoms.  We
%standardize both the features and responses so that they lie in $[0,1$], and
%collect the features into vectors $X_{\ell, t} \in \R^3$, $\ell=1,\ldots,L$,
%$t=1,\ldots,T$.
%
%At each week $t = 1, 4, 7, \ldots$, 
%we fit a simple logistic model where for a fixed $t$, we compute
%\begin{equation}
%  \label{eqn:logistic-goof}
%  (\hat \alpha^{(t)}, \hat \beta^{(t)}) \in
%  \argmin_{\alpha \in \R, \beta \in \R^d}
%  ~
%  \sum_{\ell = 1}^L
%  \left[\log(1 + e^{\alpha + X_{\ell,t}^T \beta}) -
%    Y_{\ell,t+1} (\alpha + X_{\ell,t}^T \beta)\right]
%  %% \underset{\alpha \in \R, \beta \in \R^d}{\argmin} & \sum_{\ell=1}^L \big| Y_{\ell,t+1} - ( \alpha + X_{\ell,t}^T \beta ) \big|.
%  %%   \end{array}
%\end{equation}
%We treat the data at the weeks $t=2,5,8,\ldots$ as the validation set, the
%data at the remaining weeks $t=3,6,9,\ldots$ as the test set, so that at
%each time $t$ we fit the single most recent time period's data. We make
%predictions on a new example $x$ at time $t$ via the logistic link
%\begin{equation*}
%  \what{y} = \frac{e^{\hat{\alpha}^{(t)} + x^T \hat{\beta}^{(t)}}}{1 +
%    e^{\hat{\alpha}^{(t)} + x^T \hat{\beta}^{(t)}}}.
%\end{equation*}
%
%
%%% At each week $t=1,4,7,\ldots$, we fit a simple logistic model via least absolute deviations regression, i.e., for a fixed $t$, we compute
%%% % (including the global model used on the first pass that we mentioned earlier) 
%%% \begin{equation} \label{eq:LAD}
%%%     \begin{array}{ll}
%%%         (\hat \alpha^{(t)}, \hat \beta^{(t)}) \in \underset{\alpha \in \R, \beta \in \R^d}{\argmin} & \sum_{\ell=1}^L \big| Y_{\ell,t+1} - ( \alpha + X_{\ell,t}^T \beta ) \big|.
%%%     \end{array}
%%% \end{equation}
%%% Letting $\hat \mu^{(t)}(X_{\ell, t+1}) = \hat \alpha^{(t)} + X_{\ell, t+1}^T \hat \beta^{(t)}$ and
%%% \[
%%%     g(z) = \log \Big( \frac{z+c}{1-z+c} \Big)
%%% \]
%%% denote the logit link function, the simple logistic model makes a prediction at $X_{\ell, t+1}$ by forming
%%% \begin{equation*}
%%%     g^{-1} \big( \hat \mu^{(t)}(X_{\ell, t+1}) \big). \notag %\label{eq:global-model-prediction}
%%% \end{equation*}
%%% We treat the data at the weeks $t=2,5,8,\ldots$ as the validation set, the data at the remaining weeks $t=3,6,9,\ldots$ as the test set, and work with the absolute error as in \eqref{eq:LAD} as our scoring function.
%
%For our robust conformalization procedures, we consider the Kullback-Leibler
%divergence and estimate the divergence $\rho$ between weeks $1,4,7,\ldots$
%and $2,5,8,\ldots$ via regression
%(Alg.~\ref{alg:worst-direction-validation}), as well as with a nonparametric
%divergence estimator~\citep{NguyenWaJo10}; given this $\rho$ we then make
%robust predictions at the test times $t = 3, 6, \ldots$.  We compare to the
%standard split conformal methodology---which is of course not robust to
%departures from the validation distribution---but also consider the standard
%conformal methodology with the more conservative miscoverage level
%$\alpha/2$ to attain robustness to a variation distance shift of $\alpha/2$
%(recall Corollary \ref{corollary:total-variation}).  We set $\alpha = .1$
%throughout these experiments.
%
%% AATODO: need to stick this in JD's bib file:
%
%Figures \ref{fig:covid-coverage}--\ref{fig:covid-hi-lo} present the results.  From Figure \ref{fig:covid-coverage}, we can see that the standard conformal methodology (once again) fails to cover, whereas our (two) robust conformalization procedures retain validity.  These results are in line with our expectations: we expect the standard methodology to undercover as it is not robust to distributional changes, and we expect both Alg.~\ref{alg:worst-direction-validation} as well as the nonparametric divergence estimator of \citet{NguyenWaJo10} to deliver reasonably accurate estimates of the divergence level $\rho$ given the low ambient dimension of the feature space (recall that $d=3$), translating into generally good coverage here.  We can also see that the standard conformal methodology with the conservative miscoverage level $\alpha/2$ gives coverage at roughly the right level, though it is does not adapt the miscoverage level to the problem at hand (as estimating an appropriate level of divergence is an important component).  Along these lines, Figure \ref{fig:covid-size} reveals a more complete picture: the heuristic also gives rise to (slightly) longer confidence intervals than most of the other methods---which is intuitive as again we have no guarantee that $\alpha/2$ corresponds to the true amount of divergence between the validation and test distributions.  Overall, our robust conformalization procedure combined with Nguyen et al.'s  nonparametric divergence estimator~\citep{NguyenWaJo10} appears to strike the best balance between coverage and confidence interval length in this instance.
%
%\begin{figure}[ht]
%  \centering
%  \includegraphics[width=0.9\linewidth]{covid/covid_rob_pred_ints_boxplot_Coverage_2021-10-05.pdf}
%  \caption{Empirical coverage for the prediction sets generated by the standard conformal methodology (``SC''), the standard conformal methodology where we simply set $\alpha/2$ (``SC-$\alpha/2$''), and the Kullback-Leibler divergence on the COVID-19 time series.  We set $\rho$ according to the regression-based strategy (``KL-R'') for estimating the amount of shift that we describe in Section~\ref{sec:coverage-high-probability-over-shifts}, as well as via the nonparametric divergence estimator due to \citet{NguyenWaJo10} (``KL-M2'').  The horizontal red line marks the marginal coverage $.9$.}
%  \label{fig:covid-coverage}
%\end{figure}
%
%\begin{figure}[ht]
%  \centering
%  \includegraphics[width=0.9\linewidth]{covid/covid_rob_pred_ints_boxplot_Length_2021-10-05.pdf}
%  \caption{Average length for the prediction intervals generated by the standard conformal methodology (``SC''), the standard conformal methodology where we simply set $\alpha/2$ (``SC-$\alpha/2$''), and the Kullback-Leibler divergence on the COVID-19 time series.  We set $\rho$ according to the regression-based strategy (``KL-R'') for estimating the amount of shift, as well as via the nonparametric divergence estimator due to \citet{NguyenWaJo10} (``KL-M2'').}
%  \label{fig:covid-size}
%\end{figure}
%
%We view these results from a more qualitative perspective in Figures \ref{fig:covid-true} and \ref{fig:covid-hi-lo}.  In Figure \ref{fig:covid-true}, we show the actual number of COVID-19 cases on April 16, 2021, when the state of Michigan saw a sudden spike in the incidence of COVID-19 after several weeks of implementing precautionary measures.  As an especially pronounced example of distributional shift, it is natural to ask whether our procedures might offer any kind of protection in this instance.  Figure \ref{fig:covid-hi-lo} shows the upper and lower endpoints of the confidence intervals that our robust conformalization procedure generates at this point in time.  By comparing the colors in the figures, we can see that our robust prediction intervals generally contain the true response value both across the United States as well as in Michigan, in particular---despite the presence of such a severe distributional shift.
%
%\begin{figure}[ht!]
%  \centering
%  \includegraphics[width=0.9\linewidth]{covid/covid_rob_pred_ints_04-15-2021_true_alg_idx_0_cbar.pdf}
%  \caption{The true (normalized) number of COVID-19 cases per 100{,}000 people, smoothed over the previous week, across the United States on April 16, 2021.}
%  \label{fig:covid-true}
%\end{figure}
%
%\clearpage
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=0.9\linewidth]{covid/covid_rob_pred_ints_04-15-2021_his_alg_idx_0_cbar.pdf} \\
%  \includegraphics[width=0.9\linewidth]{covid/covid_rob_pred_ints_04-15-2021_los_alg_idx_0_cbar.pdf}
%  \caption{The upper (top panel) and lower (bottom panel) endpoints of the confidence intervals that our robust conformal methodology generates across the United States on April 16, 2021.}
%  \label{fig:covid-hi-lo}
%\end{figure}
%\clearpage
%
%\subsection{Experiments on covariate sensitivity}
%\label{sec:covariate-sensitivity}
%
%Our final experiment is to evaluate our sensitivity predictions for
%covariate shift, as in Sec.~\ref{sec:sensitivity}. The point is twofold: we
%(i) identify covariates for which covarage may be sensitive, then (ii) test
%whether these putative sensitivities are indeed present in data. To do so,
%we consider three datasets from the UCI repository \citep{DuaGr17}:
%real-estate data, weather history data, and wine quality data.
%
%We repeat the following experiment 25 times:
%we randomly partition each
%dataset into disjoint sets $D_\train, D_\val, D_{\text{sens}}, D_\test$ each
%containing respectively $40\%, 10\%,30\%,10\%$ of the data, then fit a
%linear regression model $\mu$ using $D_\train$ and construct conformal
%intervals of the form~\eqref{eqn:confidence-set} with $\score(x, y) =
%|\mu(x)- y|$, so that $\what{C}_n(x) = \{y \in \R \mid |\mu(x) - y| \le
%\hat{t}\}$, setting the threshold $\hat{t}$ so that we achieve coverage at
%nominal level $\alpha = .1$ on $D_\val$.  We estimate the sensitivity
%function using $D_{\text{sens}}$ as in Algorithm~\ref{alg:sensitivity1}
%for each singleton covariate (i.e.\ the covariate set $I = \{i\}$ for
%each of $i = 1, 2, \ldots$),
%where we estimate the conditional probabilities of miscoverage using default
%tuning parameeters in R's version of random forests.
%
%Figure~\ref{fig:sens-fig} shows the results. The plot is somewhat complex:
%for each of the three datasets, we estimate sensitivity (as a function of
%shift $\rho$) for each covariate in the dataset (e.g.\ \texttt{House age} in
%the real estate data). Then for an individual covariate, we plot (estimated)
%maximum miscoverage as a function of the radius $\rho$ of potential shift in
%that covariate (the estimated sensitivity function~\eqref{eqn:covsens-fcn},
%where $I = \{i\}$ is the covariate of interest); this is the red solid line
%in each plot. As we are curious about coverage losses under covariate
%shifts, we plot miscoverage (dashed lines) on the subset of the test data
%$D_\test$ containing examples either from the upper or lower $e^{-\rho}$
%quantiles of each covariate, which corresponds to R\'{e}nyi
%$\infty$-divergence $\rho$, as in Lemma~\ref{lemma:cvar-calc}.  We expect
%that these miscoverages to fall below the maximum miscoverage line,
%which we observe across all three datasets.
%Specifically, we see that for real estate data, coverage of the
%corresponding confidence sets drops most when the marginal distribution of
%the covariate ``House age'' shifts while that for weather history data, the
%coverage drops most for shifts in the ``Pressure'' covariate.  For the wine
%quality dataset, coverage seems almost equally sensitive to all covariates.
%An interesting question for future work is to identify those directions
%which \emph{are} sensitive---as opposed to the approach here, which
%identifies potentially sensitive covariates.
%
%\begin{figure}
%  \centering
%\begin{overpic}[
%  			   %grid, %		
%  				scale=0.28]{%
%     sensitivity/real-estate.pdf}
%  \put(40, -1){
%        \small Real estate data}
% \put(-1,10){
%      \tikz{\path[draw=white, fill=white] (0, 0) rectangle (.3cm, 6cm)}
%    }
%   \put(-1,25){\rotatebox{90}{
%      $\rho$}
%    }
%  \end{overpic}
%
%%  \vspace{0.5in}
%    \centering
%  \begin{overpic}[
%  			   %grid, %		
%  				scale=0.28]{%
%     sensitivity/weather-history.pdf}
%  \put(40, -1){
%        \small Weather history data }
% \put(-1,10){
%      \tikz{\path[draw=white, fill=white] (0, 0) rectangle (.3cm, 6cm)}
%    }
%   \put(-1,20){\rotatebox{90}{
%      $\rho$}
%    }
%  \end{overpic}
%
%%  \vspace{0.5in}
%    \centering
%    \begin{overpic}[
%  			   %grid, %		
%  				scale=0.28]{%
%     sensitivity/wine-quality.pdf}
%  \put(40, -1){
%        \small Wine quality data }
% \put(-1,10){
%      \tikz{\path[draw=white, fill=white] (0, 0) rectangle (.3cm, 6cm)}
%    }
%   \put(-1,20){\rotatebox{90}{
%      $\rho$}
%    }
%  \end{overpic}
%
%  \vspace{0.2in}
% 
%  \caption{Sensitivity of (mis)-coverage for three datasets. Red line shows maximum miscoverage possible within a given shift in marginal distribution of a covariate with respect to limiting $f$-divergences. Dashed lines show miscoverage on a subset of test data that contains samples for which the corresponding covariate takes values in the upper or lower $e^{-\rho}$ quantiles of that covariate.}
%  \label{fig:sens-fig}
%\end{figure}
