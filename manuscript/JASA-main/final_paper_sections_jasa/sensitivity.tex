% -*- Mode: latex -*- %

\newcommand{\predsetthresh}{t}
\newcommand{\infdiv}[2]{D_{\infty}\left({#1} |\!| {#2}\right)}
\newcommand{\Aug}{\textrm{A}}
\newcommand{\nBatch}{B}
\newcommand{\batch}{b}
\newcommand{\qfunc}{\mc{Q}}

\newcommand{\Pcovset}{\mc{P}_{\textup{cov},I}}

\section{Coverage sensitivity under covariate shifts}
\label{sec:sensitivity}

To this point in the paper, the approaches we take for robustness to
distribution shifts may often be conservative. Here, we take a complementary
and exploratory viewpoint to identify ways in which a predictor may be
sensitive. While coverage guarantees of standard predictive inference
methods may fail when new data comes from a shifted distribution (recall
Section~\ref{sec:motivation-exp}), protecting against all possible shifts
can lead to conservative predictive sets.  It is thus of practical interest
to identify the particular directions in which a predictive model is indeed
distributionally unstable.  We therefore propose a measure that evaluates
coverage sensitivity under distribution shifts of interest, and we study
this measure's convergence properties by building on a recent line of work
distribution shift sensitivity~\citep{JeongNa20, SubbaswamyAdSa21,
  GuptaRo21}.

%\citet{JeongNa20} consider worst case average treatment effect within a given shift under covariate shift setting.
%We can extend its ideas to understand stability of the average treatment by simply looking at the worst achievable value within varying amount of shift.  \citet{SubbaswamyAdSa21} similarly consider worst case predictive risk where they consider shifts in user-defined conditional distributions with respect to the limiting $f$-divergences.
%\citet{GuptaRo21} consider evaluating the stability of general statistical parameters with respect to generic or variable specific KL-divergence based distributional shifts.
%
%\maxcomment{I don't think these three above sentences should be here, maybe in related work}

For a choice of threshold $\predsetthresh \in \R$, we wish to understand the
sensitivity of (mis)-coverage of the predictive set $C^{(\predsetthresh)}$
(as in Eq.~\eqref{eqn:confidence-set-from-score}) under covariate specific
distribution shifts.  In distinction from
Section~\ref{sec:robustpredinference}, where we consider general shifts on
the score distribution, we now focus on covariate shift.
For an index set $I \subset [d]$,
this consists
of allowing only the distribution of $X_I$ to
vary while the conditional distribution of $\score(X,Y)$ given $X_I$
remains invariant.  Thus, if we let $P_{0,I}$ be the distribution
of $(\score(X,Y), X_I)$ when $(X,Y) \sim Q_0$, we consider shifts
of measures on $(S, X_I)$ belonging to
\begin{align*}
  \Pcovset(\rho, P_{0,I}) \defeq \left\{ P
  = \law(S, X_I)
  ~ \mid ~
  P(S \in \cdot \mid X_I) = P_{0,I}(S \in \cdot \mid X_I) \text{ and } \fdiv{P}{P_{0,I}} \le \rho \right\}.
\end{align*}
Assuming (as we will show is possible) that we can accurately
evaluate coverage under such shifts, if a given scoring function $\score$ is
insensitive, then we gain confidence in the performance of $\score$,
while scoring functions sensitive to such covariate shifts should give
us pause.

The challenge of calibrating the expected distribution shift, denoted as $\rho$, is akin to calibrating sensitivity parameters in causal inference \citep{HsuSm13,VeitchZa20}. Our methods identify and assess the sensitivity of coverage to shifts in specific covariate subsets. While training data alone can not provide such calibration, access to relevant test covariate subsets can help us approximate these shifts using techniques like \citep{NguyenWaJo10}, requiring only subset data rather than full labeling—a practical advantage in many cases.

%A burning question on how to calibrate the expected shift in distribution $\rho$ still remains in practice. We note that in causal inference, a similar problem exists with calibrating the sensitivity parameter \citep{HsuSm13,VeitchZa20} where the influence of unmeasured confounders is gauged by comparing them to observed covariates. In our work, should a covariate subset risk a distribution shift that undermines coverage, methods developed in this section identify these shifts and assess coverage sensitivity. While solely training data can't calibrate distribution shifts, accessing relevant test covariate subsets can aid calibration. Using methods like \citep{NguyenWaJo10}, we can estimate covariate distribution shifts, noting that only data on sensitive subsets, not full labeling, is required—a realistic condition in many practical cases.
\subsection{Covariate-specific sensitivity analysis}

\newcommand{\SFcov}{{\SF}_{\textup{cov}, I}^{(\predsetthresh)} }
\newcommand{\estSFcov}{{\what{\SF}}_{\textup{cov}, I}^{(\predsetthresh)} }
\newcommand{\MCov}{{\MC}_{P_{0,I}}^{(\predsetthresh)}}

Our goal here is to estimate scoring model's \emph{sensitivity}, which we
take to be the mis-coverage of the predictive set function
$C^{(\predsetthresh)}$ as the distribution of $(S, X_I)$ varies within
$\Pcovset(\rho, P_{0,I})$.
For shift amounts $\rho \ge 0$ and probability distributions
(indexed by $I$) $P_{0,I}$ on $\R \times \R^I$,
we therefore define the covariate specific sensitivity function
\begin{align*}
  \SFcov (\rho,P_{0,I}) \defeq \sup_P \left\{  \E_{P}[\indic{S >
  \predsetthresh}] \mid P \in
  \Pcovset(\rho, P_{0,I}) \right\}.
\end{align*}
Define the conditional miscoverage function on $\R^I$ by
\begin{equation}
  \label{eqn:conditional-miscoverage-func}
  \MCov (x) \defeq \E_{P_{0,I}}[\indic{S>\predsetthresh} \mid X_I=x],
\end{equation}
so we can express the sensitivity function as
\begin{align}
  \label{eqn:covsens-fcn}
  \SFcov (\rho, P_{0,I}) = \sup \left\{ \E_{P}[ \MCov(X_I) ] \mid  P \in \Pcovset(\rho, P_{0,I}) \right\},
\end{align}
as the covariate shift only affects the marginal distribution of $X_I \in
\R^I$ by assumption.

The goal is to leverage equation~\eqref{eqn:covsens-fcn} to build a
consistent estimator of the sensitivity function.  Given a sample $\{
\scorerv_i, X_{I,i} \}_{i=1}^n \simiid P_{0,I}$, a natural approach is to
follow a two step procedure, by first computing an estimate
$\what{\MC}_{n_1}^{(\predsetthresh)}$ of the miscoverage function using the
first $n_1$ points, and then approximating the sensitivity function with the
remaining $n_2 = n-n_1$ data points, forming the naive estimate
\begin{equation*}
  \what{\SF}_{\textup{naive},I}(\rho)
  \defeq \sup_{Q} \left\{ \E_{X_I \sim Q}\big[\what{\MC}_{n_1}^{(\predsetthresh)}(X_I)\big]
  \text{ s.t. } \fdivs{Q}{\hat{Q}_{n_2, I}}  \leq \rho \right\},
\end{equation*}
where $\hat{Q}_{n_2, I}$ is the empirical distribution of $\{ X_{I,i}
\}_{n_1 < i \le n}$.

Unfortunately, if
$\what{\MC}_{n_1}^{(\predsetthresh)}$ converges to
$\MC_{P_{0,I}}^{(\predsetthresh)}$ at a slower rate than $\sqrt{n}$, we
expect the same behavior from $\what\SF_{\textup{naive}, I}$, so we take a
different tack.
%
In the next section, we show instead how, given an additional (large)
sample of \emph{unlabeled} data $\{X_i\}_{i = 1}^N$, we can achieve a
$\sqrt{n}$-consistent asymptotically normal estimate of $\SFcov$ using a
debiasing correction~\citep{ChernozhukovChDeDuHaNeRo16,
  JeongNa20,SubbaswamyAdSa21}.\footnote{\label{footnote:hong-wrong}
Notably, \citet{JeongNa20} and
\citet{SubbaswamyAdSa21} perform sensitivity analyses to distribution shift
for various semiparametric functionals related to that here. We present
alternative results and proofs as their results appear to have incorrect
proofs. \citet[Thm.~1]{SubbaswamyAdSa21} builds off of \citet[Lemma
  14]{JeongNa20}, whose proof~\cite[Appendix C.3]{JeongNa20} appears to have
a mistake: in the final line of the proof, they use that their functionals
$\mu : \mc{X} \to \R$ of interest have densities uniformly bounded away from
0, but nowhere do they assume this or argue that it must hold.}
A trade-off is that our debiasing typically leads to a loss
of monotonicity of the
estimate $\what{\SF}_{\text{cov}, I, n}^{(\predsetthresh)}$ in the parameter
$\rho \ge 0$. For
clarity we focus on a particular limiting divergence, the
R\'{e}nyi $\infty$-divergence
\begin{align*}
  \infdiv{P}{Q} \defeq \lim_{k \to \infty} \frac{1}{k-1} \log \left\{ \int \left( \frac{dP(z)}{dQ(z)} \right)^k dQ(z) \right\} = \log \esssup_Q \left\{ \frac{dP}{dQ} \right\}.
\end{align*}
A quick calculation shows this corresponds to distribution balls of the form  
\begin{align*}
  \{ P : \infdiv{P}{P_{0,I}} \le \rho \} = \{P \mid
  \text{ there exists } P_{1, I}  \text{ s.t. } P_{0,I} = e^{-\rho} P +(1-e^{-\rho})P_{1,I} \},
\end{align*}
which offers a simpler dual representation for the sensitivity
function~\eqref{eqn:covsens-fcn}:
\begin{lemma}[Example 3, \citet{DuchiNa21}]
  \label{lemma:cvar-calc}
  Let $\Pcovset$ be defined via the
  R\'{e}nyi divergence $D_\infty$.
  Then the sensitivity function $\SFcov (\rho, P_{0,I})$ satisfies
  \begin{align*}
    \SFcov (\rho, P_{0,I}) =\inf_{\eta \in \R} \bigg\{
    e^{\rho}\E_{P_{0,I}}
    \left[
      \hinge{\MCov(X_I)-\eta}
      \right] +\eta
    \bigg\}.
  \end{align*}
\end{lemma}

\subsection{Cross-fit dual estimation of the sensitivity function}
\label{subsec:sens-f-inf-divergence}

In the general shift case, the finite sample estimator $\SF_\text{gen}(\rho,Q,\hat{P}_n)$ is $\sqrt{n}$-consistent for $\SF_\text{gen}(\rho,Q,P_0)$,  hence we wish to construct an estimator with an analogous guarantee for the covariate specific sensitivity function $\SFcov(\rho, P_{0,I})$.

For any pair of functions $h : \R^+ \times \R^I \to
\R$ and $m: \R^I \to \R$, define the augmentation function
$\Aug^{(\predsetthresh)}_{h,m} : \R_+ \times \R \times \R^I \to \R$ by
\begin{align*}
  \Aug^{(\predsetthresh)}_{h,m}(\rho,  s,  x)
  \defeq h(\rho, x) \left(\indic{s>\predsetthresh}- m(x) \right).
\end{align*}
Let $\qfunc_0(m,\rho) \defeq \argmin_{\eta \in \R}
\big\{e^{\rho}\E[\hinge{m(X_I)-\eta}] +\eta \big\}$ be the $1-e^{-\rho}$
quantile of $m(X)$ under $P_{0,I}$.  For shorthand, omit the subscript on
the miscoverage~\eqref{eqn:conditional-miscoverage-func} to write
${\MC}^{(\predsetthresh)} \equiv \MCov$, define $\qfunc^{(\predsetthresh)}(\rho)
\defeq \qfunc_0({\MC}^{(\predsetthresh)},\rho)$, and choose
$h^{(\predsetthresh)}(\rho, x) \defeq
e^{\rho}\indics{{\MC}^{(\predsetthresh)}(x)>
  \qfunc^{(\predsetthresh)}(\rho)}$.  Then
$\E_{P_{0,I}}[\Aug^{(\predsetthresh)}_{h^{(\predsetthresh)},
    {\MC}^{(\predsetthresh)}}(\rho, \scorerv, X_I)] = 0$, so for all $\rho
>0$, Lemma~\ref{lemma:cvar-calc} shows that
\begin{align}
  \label{eqn:covsens-fcn-aug}
  \SFcov (\rho, P_{0,I}) &= e^{\rho} \E\left[
    \hinge{{\MC}^{(\predsetthresh)}(X_I)- \qfunc^{(\predsetthresh)}(\rho)}\right]
  +\qfunc^{(\predsetthresh)}(\rho) +
  \E[\Aug^{(\predsetthresh)}_{h^{(\predsetthresh)}, {\MC}^{(\predsetthresh)}}(\rho, \scorerv,  X_I)].
\end{align}

Algorithm~\ref{alg:sensitivity1} proceeds by first estimating
$\MC^{(\predsetthresh)}$, $\qfunc^{(\predsetthresh)}$ and
$h^{(\predsetthresh)}$ successively, before leveraging
equation~\eqref{eqn:covsens-fcn-aug} to form a ``debiased" cross-fit
estimator of $\SFcov (\rho, P_{0,I})$.  As mentioned above, it assumes
access to a set of unlabeled examples $\{ X_{I,j} \}_{1\le j \le N}$ where
$N \gg n$, which we use to estimate $\qfunc^{(\predsetthresh)}$ from
${\MC}^{(\predsetthresh)}$. Intuitively, this allows us to accurately
estimate properties of $\sigma(X_I)$-measurable variables, and it is
reasonable in semi-supervised regimes where unsupervised examples are cheaper than labeled data.  Appendix~\ref{sec:cross-fit-sketch-proof-intuition} provides additional intuition on the introduction of the augmentation term $\Aug^{(\predsetthresh)}_{h,m}$.

\begin{algorithm}
  \caption{Covariate sensitivity estimation}
  \label{alg:sensitivity1}
  \begin{algorithmic}
    \STATE {\bf Input:} $\nBatch$-fold partition $\cup_{\batch=1}^\nBatch \mc{I}_\batch=[n]$ of $\{(S_i,X_{I,i})\}_{i=1}^n$ s.t. $\left| \mc{I}_\batch \right| =\frac{n}{\nBatch}$, unlabeled samples $\{ X_{I, j} \}_{1\le j \le N}$, fitting procedure $\mc{A} : \left( \R \times \R^I \right)^\star \to \{ \R^I \to \R\}$.

 \FOR{$\batch \in [\nBatch]$}

  \STATE Fit an estimator
$
  \what {\MC}^{(\predsetthresh)}_\batch \defeq \mc{A}\left( (S_i,X_{I,i})_{i \in \mc{I}_\batch^c} \right)
$ of the miscoverage function ${\MC}^{(\predsetthresh)}$.

  \STATE Compute the $e^{-\rho}$-approximate quantile of $\what {\MC}^{(\predsetthresh)}_\batch$ as
  \begin{equation}
    \label{eqn:quantile-est-unlabeled}
    \what \qfunc_\batch^{(\predsetthresh)}(\rho) \defeq
    \argmin_{\eta \in \R} \left\{ \sum_{j = 1}^N
    \frac{e^{\rho}}{N}
    \hinge{\what {\MC}^{(\predsetthresh)}_\batch(X_{I,j})-\eta} + \eta \right\}.
  \end{equation}

  \STATE Set $\what h^{(\predsetthresh)}_\batch(\rho, x) \defeq e^\rho \indics{ \what {\MC}^{(\predsetthresh)}_\batch(x) > \what \qfunc_\batch^{(\predsetthresh)}(\rho)}$.

    \STATE Compute the $b$-th fold augmented estimator
    \begin{equation}
      \label{eqn:sens-func-aug-estimator}
      \what{\SF}^{(q)}_{\batch,n}(\rho) \defeq  \frac{1}{\left| \mc{I}_\batch \right|} \sum_{i \in \mc{I}_\batch}  \left\{ e^{\rho}
      \hinge{\what{\MC}^{(\predsetthresh)}_\batch(X_{I,i})- \what \qfunc_\batch^{(\predsetthresh)}(\rho)} + \Aug^{(\predsetthresh)}_{\what h^{(\predsetthresh)}_\batch, \what {\MC}^{(\predsetthresh)}_\batch}(\rho, \scorerv_i, X_{I,i})  \right\} +  \what \qfunc_\batch^{(\predsetthresh)}(\rho).
    \end{equation}

   \ENDFOR
   \RETURN
    \begin{align}
    \label{eqn:final-sens-aug-estimator}
    \what{\SF}^{(\predsetthresh)}_n(\rho) \defeq \frac{1}{\nBatch} \sum_{\batch=1}^\nBatch \what{\SF}^{(\predsetthresh)}_{\batch,n}(\rho)
    \end{align}

  \end{algorithmic}
\end{algorithm}

%\subsubsection{Consistency and convergence rate of the augmented estimator $ \what{\SF}^{(q)}_n$}
%
%We study the consistency and rate of covergence of the sensitivity
%estimator~\eqref{eqn:final-sens-aug-estimator} as a path function of $\rho
%\in \R_+$, for which we require a few assumptions below.
%Assumption~\ref{ass:miscov-estimator-consistent} states that the fitted
%estimator $\what \MC_\batch$ needs to be appropriately consistent for
%$\MCov$, while Assumption~\ref{ass:miscov-quantile-estimator-consistent}
%basically ensures the pool of unlabeled samples is large enough to provide a
%good estimate of the quantiles of $\what \MC_\batch$.
%Assumption~\ref{ass:miscov-density} is technical and prevents
%the random variable $\MCov(X_I)$ from being too concentrated,
%thus allowing quantile estimation.
%%
%%\textbf{Notation}. Let $P^{-1}_{\alpha}(p)$ denote the $\alpha$-th quantile of random variable $p(I)$. Further, when $p(\cdot)$ is random, the probabilities are only taken over $X_I \sim P_{0,I}$.
%
%\begin{assumption}[Miscoverage estimation]
%  \label{ass:miscov-estimator-consistent}
%  For each batch $\batch \in [\nBatch]$,
%  we have
%  \begin{align*}
%    \norm{\what {\MC}^{(\predsetthresh)}_\batch- {\MC}^{(\predsetthresh)}}_{L^\infty(P_{0,I})} = o_p(n^{-1/4}).
%  \end{align*}
%\end{assumption}
%
%\begin{assumption}[Quantile estimation]
%  \label{ass:miscov-quantile-estimator-consistent}
%  For each $\batch \in [\nBatch]$ and every
%  compact $K \subset \R_+$,
%  the quantile estimator $\what \qfunc_\batch$ satisfies
%  \begin{align*}
%    \sup_{\rho \in K} \left| \what \qfunc_\batch(\rho) - \qfunc_0(\what \MC_\batch, \rho)\right| = o_p(n^{-1/4}).
%  \end{align*}
%\end{assumption}
%
%\begin{assumption}
%\label{ass:miscov-density}
%The random variable $\MCov(X_I)$ has a bounded density $f_{\MC}$ on $[0,1]$.
%\end{assumption}
%
%Under these assumptions, we have the following
%theorem, which shows that for every compact set $K \subset \R$, the sequence of
%processes $\{ \sqrt{n}(\what{\SF}^{(\predsetthresh)}_n(\rho)-
%\SFcov(\rho)) \}_{\rho \in K}$ converges in distribution in $L^\infty(K)$ to
%a tight Gaussian process, whose covariance is tedious to specify
%though we characterize it in the proof of the theorem in
%Appendix~\ref{proof-thm-unif-conv-sens}.
%\begin{theorem}
%  \label{thm:unif-conv-sens}
%  Let
%  Assumptions~\ref{ass:miscov-estimator-consistent}--\ref{ass:miscov-density}
%  hold. Then there exists a tight Gaussian process $\mathbb{G}$ such that,
%  for every compact set $K \subset \R^+$, we have
%  \begin{align*}
%    \{ \sqrt{n}(\what{\SF}^{(\predsetthresh)}_n(\rho)- \SFcov(\rho)) \}_{\rho \in K}
%    \cd
%    \{ \mathbb{G}(\rho) \}_{\rho \in K}
%    ~~~ \mbox{as~elements~of~~}L^\infty(K).
%  \end{align*}
%\end{theorem}
%
%A few consequences of Theorem~\ref{thm:unif-conv-sens} are immediate.
%First, we have $\sqrt{n}$-consistency:
%\begin{align*}
%  \sqrt{n} \cdot
%  \sup_{\rho \in K}|\what{\SF}^{(\predsetthresh)}_n(\rho)- \SFcov(\rho))|
%  \cd \sup_{\rho \in K} \mathbb{G}(\rho)
%\end{align*}
%as the supremum mapping is continuous in $L^\infty$, and so
%$\linfs{\what{\SF}^{(t)}_n - \SFcov} = O_P(1/\sqrt{n})$. As another
%immediate consequence, we see that under the assumptions of
%Theorem~\ref{thm:unif-conv-sens},
%for every $\rho >0$, there exists $\sigma^2(\rho) < \infty$ such that
%\begin{align*}
%  \sqrt{n}(\what{\SF}^{(\predsetthresh)}_n(\rho)- \SFcov(\rho)) \cd
%   \normal(0,\sigma^2(\rho)).
%\end{align*}
%(This is similar to the result \cite[Thm.~1]{SubbaswamyAdSa21},
%but as we note in footnote~\ref{footnote:hong-wrong},
%the papers~\cite{SubbaswamyAdSa21,JeongNa20} may have technical
%mistakes.)
